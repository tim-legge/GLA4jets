{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "### First attempted implementation of Gated Linear Attention\n",
    "### We will load a pretrained language model and try to adapt its usage\n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "# Copyright (c) 2023-2025, Songlin Yang, Yu Zhang\n",
    "\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from typing import TYPE_CHECKING, Dict, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange, repeat\n",
    "\n",
    "from fla.layers.utils import get_unpad_data, index_first_axis, pad_input\n",
    "from fla.modules import FusedRMSNormGated, RMSNorm, ShortConvolution\n",
    "from fla.modules.activations import ACT2FN\n",
    "from fla.ops.gla import chunk_gla, fused_chunk_gla, fused_recurrent_gla\n",
    "\n",
    "if TYPE_CHECKING:\n",
    "    from transformers.processing_utils import Unpack\n",
    "\n",
    "    from fla.models.utils import Cache\n",
    "\n",
    "\n",
    "class GatedLinearAttention(nn.Module):\n",
    "    r\"\"\"\n",
    "    The layer implementaion for [Gated Linear Attention Transformers with Hardware-Efficient Training](https://arxiv.org/abs/2312.06635).  # noqa\n",
    "\n",
    "    Args:\n",
    "        mode (str, Optional):\n",
    "            Which GLA kernel to use.\n",
    "            Currently available: `chunk`, `fused_recurrent`, and `fused_chunk`.\n",
    "            Default: `chunk`.\n",
    "        hidden_size (int, Optional):\n",
    "            The hidden size of the input. Default: 1024.\n",
    "        expand_k (float, Optional):\n",
    "            The expansion ratio for the key dim. Default: 0.5.\n",
    "        expand_v (float, Optional):\n",
    "            The expansion ratio for the value dim. Default: 1.0.\n",
    "        num_heads (int, Optional):\n",
    "            The number of heads. Default: 4.\n",
    "        num_kv_heads (int, Optional):\n",
    "            The number of key/value heads, used for MQA. Default: None.\n",
    "        feature_map (str, Optional):\n",
    "            Feature map function applied to queries/keys. Default: None.\n",
    "        use_short_conv (bool, Optional):\n",
    "            Whether to use short convolutions. Default: `False`.\n",
    "        conv_size (int, Optional):\n",
    "            The kernel size of the short convolution, only used when `use_short_conv` is `True`. Default: 4.\n",
    "        conv_bias (bool, Optional):\n",
    "            Whether to use bias in the short convolution, only used when `use_short_conv` is `True`. Default: `False`.\n",
    "        use_output_gate (bool, Optional):\n",
    "            Whether to use output gate. Default: `True`.\n",
    "        gate_fn (str, Optional):\n",
    "            The activation function for the output gate. Default: `swish`.\n",
    "        elementwise_affine (bool, Optional):\n",
    "            If `True`, applies elementwise affine to LayerNorm with learnable parameters. Default: `True`.\n",
    "        norm_eps (float, Optional):\n",
    "            The epsilon value for the layernorm/rmsnorm layer. Default: 1e-5.\n",
    "        gate_logit_normalizer (int, Optional):\n",
    "            The normalizer for the gate logits, appied after `logsigmoid`. Default: 16.\n",
    "        gate_low_rank_dim (int, Optional):\n",
    "            The low rank dim for the gate projection. Default: 16.\n",
    "        clamp_min (float, Optional):\n",
    "            The minimum value for the gate logits. Default: None.\n",
    "        fuse_norm (bool, Optional):\n",
    "            Whether to fuse the norm and the output gate for better memory footprint. Default: `True`.\n",
    "        layer_idx (int, Optional):\n",
    "            The index of the layer. Default: None.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        mode: str = 'chunk',\n",
    "        hidden_size: int = 1024,\n",
    "        expand_k: float = 0.5,\n",
    "        expand_v: float = 1.0,\n",
    "        num_heads: int = 4,\n",
    "        num_kv_heads: Optional[int] = None,\n",
    "        feature_map: Optional[str] = None,\n",
    "        use_short_conv: bool = False,\n",
    "        conv_size: int = 4,\n",
    "        conv_bias: bool = False,\n",
    "        use_output_gate: bool = True,\n",
    "        gate_fn: str = 'swish',\n",
    "        elementwise_affine: Optional[bool] = True,\n",
    "        norm_eps: float = 1e-5,\n",
    "        gate_logit_normalizer: int = 16,\n",
    "        gate_low_rank_dim: int = 16,\n",
    "        clamp_min: Optional[float] = None,\n",
    "        fuse_norm: bool = True,\n",
    "        layer_idx: int = None,\n",
    "    ) -> GatedLinearAttention:\n",
    "        super().__init__()\n",
    "\n",
    "        self.mode = mode\n",
    "        self.hidden_size = hidden_size\n",
    "        self.expand_k = expand_k\n",
    "        self.expand_v = expand_v\n",
    "        self.num_heads = num_heads\n",
    "        self.num_kv_heads = num_kv_heads if num_kv_heads is not None else num_heads\n",
    "        self.num_kv_groups = self.num_heads // self.num_kv_heads\n",
    "        self.feature_map_fn = ACT2FN[feature_map] if feature_map is not None else None\n",
    "\n",
    "        self.use_short_conv = use_short_conv\n",
    "        self.conv_size = conv_size\n",
    "        self.conv_bias = conv_bias\n",
    "        self.use_output_gate = use_output_gate\n",
    "\n",
    "        self.key_dim = int(hidden_size * expand_k)\n",
    "        self.value_dim = int(hidden_size * expand_v)\n",
    "        self.key_dim_per_group = self.key_dim // self.num_kv_groups\n",
    "        self.value_dim_per_group = self.value_dim // self.num_kv_groups\n",
    "        self.clamp_min = clamp_min\n",
    "        self.layer_idx = layer_idx\n",
    "\n",
    "        assert mode in ['chunk', 'fused_recurrent', 'fused_chunk'], f\"Not supported mode `{mode}`.\"\n",
    "        assert self.key_dim % num_heads == 0, f\"key dim must be divisible by num_heads of {num_heads}\"\n",
    "        assert self.value_dim % num_heads == 0, f\"value dim must be divisible by num_heads of {num_heads}\"\n",
    "\n",
    "        self.head_k_dim = self.key_dim // num_heads\n",
    "        self.head_v_dim = self.value_dim // num_heads\n",
    "\n",
    "        self.q_proj = nn.Linear(hidden_size, self.key_dim, bias=False)\n",
    "        self.k_proj = nn.Linear(hidden_size, self.key_dim_per_group, bias=False)\n",
    "        self.v_proj = nn.Linear(hidden_size, self.value_dim_per_group, bias=False)\n",
    "        if self.use_output_gate:\n",
    "            self.g_proj = nn.Linear(hidden_size, self.value_dim, bias=False)\n",
    "\n",
    "        if use_short_conv:\n",
    "            self.conv_size = conv_size\n",
    "            self.q_conv1d = ShortConvolution(\n",
    "                hidden_size=self.key_dim,\n",
    "                kernel_size=conv_size,\n",
    "                bias=conv_bias,\n",
    "                activation='silu',\n",
    "            )\n",
    "            self.k_conv1d = ShortConvolution(\n",
    "                hidden_size=self.key_dim_per_group,\n",
    "                kernel_size=conv_size,\n",
    "                bias=conv_bias,\n",
    "                activation='silu',\n",
    "            )\n",
    "            self.v_conv1d = ShortConvolution(\n",
    "                hidden_size=self.value_dim_per_group,\n",
    "                kernel_size=conv_size,\n",
    "                bias=conv_bias,\n",
    "                activation='silu',\n",
    "            )\n",
    "\n",
    "        self.gk_proj = nn.Sequential(nn.Linear(hidden_size, gate_low_rank_dim, bias=False),\n",
    "                                     nn.Linear(gate_low_rank_dim, self.key_dim_per_group, bias=True))\n",
    "        self.o_proj = nn.Linear(self.value_dim, hidden_size, bias=False)\n",
    "\n",
    "        if gate_fn == 'swish' and fuse_norm and use_output_gate:\n",
    "            self.g_norm_swish_gate = FusedRMSNormGated(\n",
    "                hidden_size=self.head_v_dim,\n",
    "                elementwise_affine=elementwise_affine,\n",
    "                eps=norm_eps\n",
    "            )\n",
    "            self.fuse_norm_and_gate = True\n",
    "        else:\n",
    "            self.fuse_norm_and_gate = False\n",
    "            self.g_norm = RMSNorm(\n",
    "                hidden_size=self.head_v_dim,\n",
    "                elementwise_affine=elementwise_affine,\n",
    "                eps=norm_eps\n",
    "            )\n",
    "            self.gate_fn = ACT2FN[gate_fn]\n",
    "\n",
    "        self.gate_logit_normalizer = gate_logit_normalizer\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        k: Optional[torch.Tensor] = None,\n",
    "        v: Optional[torch.Tensor] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        past_key_values: Optional[Cache] = None,\n",
    "        use_cache: Optional[bool] = False,\n",
    "        output_attentions: Optional[bool] = False,\n",
    "        **kwargs: Unpack[Dict]\n",
    "    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Cache]]:\n",
    "        \n",
    "        device = hidden_states.device\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            #print(attention_mask.shape)\n",
    "            #attention_mask = np.reshape(attention_mask[:,:], newshape=(attention_mask.shape[1], attention_mask.shape[-1]))\n",
    "            #print(attention_mask.shape)\n",
    "            attention_mask = attention_mask.to(device=device)\n",
    "            assert len(attention_mask.shape) == 2, (\n",
    "                \"Expected attention_mask as a 0-1 matrix with shape [batch_size, seq_len] \"\n",
    "                \"for padding purposes (0 indicating padding). \"\n",
    "                \"Arbitrary attention masks of shape [batch_size, seq_len, seq_len] are not allowed.\"\n",
    "            )\n",
    "\n",
    "        batch_size, q_len, _ = hidden_states.shape\n",
    "        mode = 'fused_recurrent' if hidden_states.shape[1] <= 64 else self.mode\n",
    "\n",
    "        last_state = None\n",
    "        if past_key_values is not None and len(past_key_values) > self.layer_idx:\n",
    "            last_state = past_key_values[self.layer_idx]\n",
    "\n",
    "        cu_seqlens = kwargs.get('cu_seqlens', None)\n",
    "        if attention_mask is not None:\n",
    "            indices, cu_seqlens, _ = get_unpad_data(attention_mask[:, -q_len:])\n",
    "            hidden_states = index_first_axis(rearrange(hidden_states, \"b s ... -> (b s) ...\"), indices).unsqueeze(0)\n",
    "\n",
    "        if self.use_short_conv:\n",
    "            conv_state_q, conv_state_k, conv_state_v = None, None, None\n",
    "            if last_state is not None:\n",
    "                conv_state_q, conv_state_k, conv_state_v = last_state['conv_state']\n",
    "            q, conv_state_q = self.q_conv1d(\n",
    "                x=self.q_proj(hidden_states),\n",
    "                cache=conv_state_q,\n",
    "                output_final_state=use_cache,\n",
    "                cu_seqlens=cu_seqlens\n",
    "            )\n",
    "            k, conv_state_k = self.k_conv1d(\n",
    "                x=self.k_proj(hidden_states),\n",
    "                cache=conv_state_k,\n",
    "                output_final_state=use_cache,\n",
    "                cu_seqlens=cu_seqlens\n",
    "            )\n",
    "            v, conv_state_v = self.v_conv1d(\n",
    "                x=self.v_proj(hidden_states),\n",
    "                cache=conv_state_v,\n",
    "                output_final_state=use_cache,\n",
    "                cu_seqlens=cu_seqlens\n",
    "            )\n",
    "        else:\n",
    "            if k is not None and v is not None: # cross attention in the class token layers\n",
    "                q = self.q_proj(hidden_states)\n",
    "                k = self.k_proj(k)\n",
    "                v = self.v_proj(v)\n",
    "            else:\n",
    "                q = self.q_proj(hidden_states)\n",
    "                k = self.k_proj(hidden_states)\n",
    "                v = self.v_proj(hidden_states)\n",
    "        gk = self.gk_proj(hidden_states)\n",
    "\n",
    "        if self.feature_map_fn is not None:\n",
    "            q, k = map(self.feature_map_fn, (q, k))\n",
    "        q = rearrange(q, '... (h d) -> ... h d', d=self.head_k_dim)\n",
    "        if self.num_kv_groups > 1:\n",
    "            k, gk = (repeat(x, '... (h d) -> ... (h g) d', g=self.num_kv_groups, d=self.head_k_dim) for x in (k, gk))\n",
    "            v = repeat(v, '... (h d) -> ... (h g) d', g=self.num_kv_groups, d=self.head_v_dim)\n",
    "        else:\n",
    "            k, gk = (rearrange(x, '... (h d) -> ... h d', d=self.head_k_dim) for x in (k, gk))\n",
    "            v = rearrange(v, '... (h d) -> ... h d', d=self.head_v_dim)\n",
    "        gk = F.logsigmoid(gk) / self.gate_logit_normalizer\n",
    "\n",
    "        if self.clamp_min is not None:\n",
    "            gk = torch.clamp_min(gk, self.clamp_min)\n",
    "\n",
    "        recurrent_state = last_state['recurrent_state'] if last_state is not None else None\n",
    "        if mode == 'fused_recurrent':\n",
    "            o, recurrent_state = fused_recurrent_gla(\n",
    "                q=q,\n",
    "                k=k,\n",
    "                v=v,\n",
    "                gk=gk,\n",
    "                initial_state=recurrent_state,\n",
    "                output_final_state=use_cache,\n",
    "                cu_seqlens=cu_seqlens,\n",
    "            )\n",
    "        elif mode == 'fused_chunk':\n",
    "            o, recurrent_state = fused_chunk_gla(\n",
    "                q=q,\n",
    "                k=k,\n",
    "                v=v,\n",
    "                g=gk,\n",
    "                initial_state=recurrent_state,\n",
    "                output_final_state=use_cache,\n",
    "            )\n",
    "        elif mode == 'chunk':\n",
    "            o, recurrent_state = chunk_gla(\n",
    "                q=q,\n",
    "                k=k,\n",
    "                v=v,\n",
    "                g=gk,\n",
    "                initial_state=recurrent_state,\n",
    "                output_final_state=use_cache,\n",
    "                cu_seqlens=cu_seqlens,\n",
    "            )\n",
    "        else:\n",
    "            raise NotImplementedError(f\"Not supported mode `{mode}`.\")\n",
    "\n",
    "        if past_key_values is not None:\n",
    "            past_key_values.update(\n",
    "                recurrent_state=recurrent_state,\n",
    "                conv_state=(conv_state_q, conv_state_k, conv_state_v) if self.use_short_conv else None,\n",
    "                layer_idx=self.layer_idx,\n",
    "                offset=q_len\n",
    "            )\n",
    "\n",
    "        if self.use_output_gate:\n",
    "            g = self.g_proj(hidden_states)\n",
    "            if self.fuse_norm_and_gate:\n",
    "                g = rearrange(g, '... (h d) -> ... h d', d=self.head_v_dim)\n",
    "                o = self.g_norm_swish_gate(o, g)\n",
    "                o = rearrange(o, '... h d -> ... (h d)')\n",
    "            else:\n",
    "                o = rearrange(self.g_norm(o), '... h d -> ... (h d)')\n",
    "                o = o * self.gate_fn(g)\n",
    "        else:\n",
    "            o = rearrange(self.g_norm(o), '... h d -> ... (h d)')\n",
    "        o = self.o_proj(o)\n",
    "        if attention_mask is not None:\n",
    "            o = pad_input(o.squeeze(0), indices, batch_size, q_len)\n",
    "\n",
    "        return o, None, past_key_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c3258b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Modified ParT to use GLA\n",
    "\n",
    "''' Particle Transformer (ParT)\n",
    "\n",
    "Paper: \"Particle Transformer for Jet Tagging\" - https://arxiv.org/abs/2202.03772\n",
    "'''\n",
    "import math\n",
    "import random\n",
    "import warnings\n",
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from functools import partial\n",
    "\n",
    "from typing import Dict, Optional, Tuple\n",
    "from torch import Tensor, nn\n",
    "from torch.nn import Parameter\n",
    "from weaver.utils.logger import _logger\n",
    "import torch.nn.functional as F\n",
    "\n",
    "@torch.jit.script\n",
    "def delta_phi(a, b):\n",
    "    return (a - b + math.pi) % (2 * math.pi) - math.pi\n",
    "\n",
    "\n",
    "@torch.jit.script\n",
    "def delta_r2(eta1, phi1, eta2, phi2):\n",
    "    return (eta1 - eta2)**2 + delta_phi(phi1, phi2)**2\n",
    "\n",
    "\n",
    "def to_pt2(x, eps=1e-8):\n",
    "    pt2 = x[:, :2].square().sum(dim=1, keepdim=True)\n",
    "    if eps is not None:\n",
    "        pt2 = pt2.clamp(min=eps)\n",
    "    return pt2\n",
    "\n",
    "\n",
    "def to_m2(x, eps=1e-8):\n",
    "    m2 = x[:, 3:4].square() - x[:, :3].square().sum(dim=1, keepdim=True)\n",
    "    if eps is not None:\n",
    "        m2 = m2.clamp(min=eps)\n",
    "    return m2\n",
    "\n",
    "\n",
    "def atan2(y, x):\n",
    "    sx = torch.sign(x)\n",
    "    sy = torch.sign(y)\n",
    "    pi_part = (sy + sx * (sy ** 2 - 1)) * (sx - 1) * (-math.pi / 2)\n",
    "    atan_part = torch.arctan(y / (x + (1 - sx ** 2))) * sx ** 2\n",
    "    return atan_part + pi_part\n",
    "\n",
    "\n",
    "def to_ptrapphim(x, return_mass=True, eps=1e-8, for_onnx=False):\n",
    "    # x: (N, 4, ...), dim1 : (px, py, pz, E)\n",
    "    px, py, pz, energy = x.split((1, 1, 1, 1), dim=1)\n",
    "    pt = torch.sqrt(to_pt2(x, eps=eps))\n",
    "    # rapidity = 0.5 * torch.log((energy + pz) / (energy - pz))\n",
    "    rapidity = 0.5 * torch.log(1 + (2 * pz) / (energy - pz).clamp(min=1e-20))\n",
    "    phi = (atan2 if for_onnx else torch.atan2)(py, px)\n",
    "    if not return_mass:\n",
    "        return torch.cat((pt, rapidity, phi), dim=1)\n",
    "    else:\n",
    "        m = torch.sqrt(to_m2(x, eps=eps))\n",
    "        return torch.cat((pt, rapidity, phi, m), dim=1)\n",
    "\n",
    "\n",
    "def boost(x, boostp4, eps=1e-8):\n",
    "    # boost x to the rest frame of boostp4\n",
    "    # x: (N, 4, ...), dim1 : (px, py, pz, E)\n",
    "    p3 = -boostp4[:, :3] / boostp4[:, 3:].clamp(min=eps)\n",
    "    b2 = p3.square().sum(dim=1, keepdim=True)\n",
    "    gamma = (1 - b2).clamp(min=eps)**(-0.5)\n",
    "    gamma2 = (gamma - 1) / b2\n",
    "    gamma2.masked_fill_(b2 == 0, 0)\n",
    "    bp = (x[:, :3] * p3).sum(dim=1, keepdim=True)\n",
    "    v = x[:, :3] + gamma2 * bp * p3 + x[:, 3:] * gamma * p3\n",
    "    return v\n",
    "\n",
    "\n",
    "def p3_norm(p, eps=1e-8):\n",
    "    return p[:, :3] / p[:, :3].norm(dim=1, keepdim=True).clamp(min=eps)\n",
    "\n",
    "\n",
    "def pairwise_lv_fts(xi, xj, num_outputs=4, eps=1e-8, for_onnx=False):\n",
    "    pti, rapi, phii = to_ptrapphim(xi, False, eps=None, for_onnx=for_onnx).split((1, 1, 1), dim=1)\n",
    "    ptj, rapj, phij = to_ptrapphim(xj, False, eps=None, for_onnx=for_onnx).split((1, 1, 1), dim=1)\n",
    "\n",
    "    delta = delta_r2(rapi, phii, rapj, phij).sqrt()\n",
    "    lndelta = torch.log(delta.clamp(min=eps))\n",
    "    if num_outputs == 1:\n",
    "        return lndelta\n",
    "\n",
    "    if num_outputs > 1:\n",
    "        ptmin = ((pti <= ptj) * pti + (pti > ptj) * ptj) if for_onnx else torch.minimum(pti, ptj)\n",
    "        lnkt = torch.log((ptmin * delta).clamp(min=eps))\n",
    "        lnz = torch.log((ptmin / (pti + ptj).clamp(min=eps)).clamp(min=eps))\n",
    "        outputs = [lnkt, lnz, lndelta]\n",
    "\n",
    "    if num_outputs > 3:\n",
    "        xij = xi + xj\n",
    "        lnm2 = torch.log(to_m2(xij, eps=eps))\n",
    "        outputs.append(lnm2)\n",
    "\n",
    "    if num_outputs > 4:\n",
    "        lnds2 = torch.log(torch.clamp(-to_m2(xi - xj, eps=None), min=eps))\n",
    "        outputs.append(lnds2)\n",
    "\n",
    "    # the following features are not symmetric for (i, j)\n",
    "    if num_outputs > 5:\n",
    "        xj_boost = boost(xj, xij)\n",
    "        costheta = (p3_norm(xj_boost, eps=eps) * p3_norm(xij, eps=eps)).sum(dim=1, keepdim=True)\n",
    "        outputs.append(costheta)\n",
    "\n",
    "    if num_outputs > 6:\n",
    "        deltarap = rapi - rapj\n",
    "        deltaphi = delta_phi(phii, phij)\n",
    "        outputs += [deltarap, deltaphi]\n",
    "\n",
    "    assert (len(outputs) == num_outputs)\n",
    "    return torch.cat(outputs, dim=1)\n",
    "\n",
    "\n",
    "def build_sparse_tensor(uu, idx, seq_len):\n",
    "    # inputs: uu (N, C, num_pairs), idx (N, 2, num_pairs)\n",
    "    # return: (N, C, seq_len, seq_len)\n",
    "    batch_size, num_fts, num_pairs = uu.size()\n",
    "    idx = torch.min(idx, torch.ones_like(idx) * seq_len)\n",
    "    i = torch.cat((\n",
    "        torch.arange(0, batch_size, device=uu.device).repeat_interleave(num_fts * num_pairs).unsqueeze(0),\n",
    "        torch.arange(0, num_fts, device=uu.device).repeat_interleave(num_pairs).repeat(batch_size).unsqueeze(0),\n",
    "        idx[:, :1, :].expand_as(uu).flatten().unsqueeze(0),\n",
    "        idx[:, 1:, :].expand_as(uu).flatten().unsqueeze(0),\n",
    "    ), dim=0)\n",
    "    return torch.sparse_coo_tensor(\n",
    "        i, uu.flatten(),\n",
    "        size=(batch_size, num_fts, seq_len + 1, seq_len + 1),\n",
    "        device=uu.device).to_dense()[:, :, :seq_len, :seq_len]\n",
    "\n",
    "\n",
    "def trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):\n",
    "    # From https://github.com/rwightman/pytorch-image-models/blob/18ec173f95aa220af753358bf860b16b6691edb2/timm/layers/weight_init.py#L8\n",
    "    r\"\"\"Fills the input Tensor with values drawn from a truncated\n",
    "    normal distribution. The values are effectively drawn from the\n",
    "    normal distribution :math:`\\mathcal{N}(\\text{mean}, \\text{std}^2)`\n",
    "    with values outside :math:`[a, b]` redrawn until they are within\n",
    "    the bounds. The method used for generating the random values works\n",
    "    best when :math:`a \\leq \\text{mean} \\leq b`.\n",
    "    Args:\n",
    "        tensor: an n-dimensional `torch.Tensor`\n",
    "        mean: the mean of the normal distribution\n",
    "        std: the standard deviation of the normal distribution\n",
    "        a: the minimum cutoff value\n",
    "        b: the maximum cutoff value\n",
    "    Examples:\n",
    "        >>> w = torch.empty(3, 5)\n",
    "        >>> nn.init.trunc_normal_(w)\n",
    "    \"\"\"\n",
    "    def norm_cdf(x):\n",
    "        # Computes standard normal cumulative distribution function\n",
    "        return (1. + math.erf(x / math.sqrt(2.))) / 2.\n",
    "\n",
    "    if (mean < a - 2 * std) or (mean > b + 2 * std):\n",
    "        warnings.warn(\"mean is more than 2 std from [a, b] in nn.init.trunc_normal_. \"\n",
    "                      \"The distribution of values may be incorrect.\",\n",
    "                      stacklevel=2)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Values are generated by using a truncated uniform distribution and\n",
    "        # then using the inverse CDF for the normal distribution.\n",
    "        # Get upper and lower cdf values\n",
    "        l = norm_cdf((a - mean) / std)\n",
    "        u = norm_cdf((b - mean) / std)\n",
    "\n",
    "        # Uniformly fill tensor with values from [l, u], then translate to\n",
    "        # [2l-1, 2u-1].\n",
    "        tensor.uniform_(2 * l - 1, 2 * u - 1)\n",
    "\n",
    "        # Use inverse cdf transform for normal distribution to get truncated\n",
    "        # standard normal\n",
    "        tensor.erfinv_()\n",
    "\n",
    "        # Transform to proper mean, std\n",
    "        tensor.mul_(std * math.sqrt(2.))\n",
    "        tensor.add_(mean)\n",
    "\n",
    "        # Clamp to ensure it's in the proper range\n",
    "        tensor.clamp_(min=a, max=b)\n",
    "        return tensor\n",
    "\n",
    "\n",
    "class SequenceTrimmer(nn.Module):\n",
    "\n",
    "    def __init__(self, enabled=False, target=(0.9, 1.02), **kwargs) -> None:\n",
    "        super().__init__(**kwargs)\n",
    "        self.enabled = enabled\n",
    "        self.target = target\n",
    "        self._counter = 0\n",
    "\n",
    "    def forward(self, x, v=None, mask=None, uu=None):\n",
    "        # x: (N, C, P)\n",
    "        # v: (N, 4, P) [px,py,pz,energy]\n",
    "        # mask: (N, 1, P) -- real particle = 1, padded = 0\n",
    "        # uu: (N, C', P, P)\n",
    "        if mask is None:\n",
    "            mask = torch.ones_like(x[:, :1])\n",
    "        mask = mask.bool()\n",
    "\n",
    "        if self.enabled:\n",
    "            if self._counter < 5:\n",
    "                self._counter += 1\n",
    "            else:\n",
    "                if self.training:\n",
    "                    q = min(1, random.uniform(*self.target))\n",
    "                    maxlen = torch.quantile(mask.type_as(x).sum(dim=-1), q).long()\n",
    "                    rand = torch.rand_like(mask.type_as(x))\n",
    "                    rand.masked_fill_(~mask, -1)\n",
    "                    perm = rand.argsort(dim=-1, descending=True)  # (N, 1, P)\n",
    "                    mask = torch.gather(mask, -1, perm)\n",
    "                    x = torch.gather(x, -1, perm.expand_as(x))\n",
    "                    if v is not None:\n",
    "                        v = torch.gather(v, -1, perm.expand_as(v))\n",
    "                    if uu is not None:\n",
    "                        uu = torch.gather(uu, -2, perm.unsqueeze(-1).expand_as(uu))\n",
    "                        uu = torch.gather(uu, -1, perm.unsqueeze(-2).expand_as(uu))\n",
    "                else:\n",
    "                    maxlen = mask.sum(dim=-1).max()\n",
    "                maxlen = max(maxlen, 1)\n",
    "                if maxlen < mask.size(-1):\n",
    "                    mask = mask[:, :, :maxlen]\n",
    "                    x = x[:, :, :maxlen]\n",
    "                    if v is not None:\n",
    "                        v = v[:, :, :maxlen]\n",
    "                    if uu is not None:\n",
    "                        uu = uu[:, :, :maxlen, :maxlen]\n",
    "\n",
    "        return x, v, mask, uu\n",
    "\n",
    "\n",
    "class Embed(nn.Module):\n",
    "    def __init__(self, input_dim, dims, normalize_input=True, activation='gelu'):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_bn = nn.BatchNorm1d(input_dim) if normalize_input else None\n",
    "        module_list = []\n",
    "        for dim in dims:\n",
    "            module_list.extend([\n",
    "                nn.LayerNorm(input_dim),\n",
    "                nn.Linear(input_dim, dim),\n",
    "                nn.GELU() if activation == 'gelu' else nn.ReLU(),\n",
    "            ])\n",
    "            input_dim = dim\n",
    "        self.embed = nn.Sequential(*module_list)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.input_bn is not None:\n",
    "            # x: (batch, embed_dim, seq_len)\n",
    "            x = self.input_bn(x)\n",
    "            x = x.permute(2, 0, 1).contiguous()\n",
    "        # x: (seq_len, batch, embed_dim)\n",
    "        return self.embed(x)\n",
    "\n",
    "\n",
    "class PairEmbed(nn.Module):\n",
    "    def __init__(\n",
    "            self, pairwise_lv_dim, pairwise_input_dim, dims,\n",
    "            remove_self_pair=False, use_pre_activation_pair=True, mode='sum',\n",
    "            normalize_input=True, activation='gelu', eps=1e-8,\n",
    "            for_onnx=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.pairwise_lv_dim = pairwise_lv_dim\n",
    "        self.pairwise_input_dim = pairwise_input_dim\n",
    "        self.is_symmetric = (pairwise_lv_dim <= 5) and (pairwise_input_dim == 0)\n",
    "        self.remove_self_pair = remove_self_pair\n",
    "        self.mode = mode\n",
    "        self.for_onnx = for_onnx\n",
    "        self.pairwise_lv_fts = partial(pairwise_lv_fts, num_outputs=pairwise_lv_dim, eps=eps, for_onnx=for_onnx)\n",
    "        self.out_dim = dims[-1]\n",
    "\n",
    "        if self.mode == 'concat':\n",
    "            input_dim = pairwise_lv_dim + pairwise_input_dim\n",
    "            module_list = [nn.BatchNorm1d(input_dim)] if normalize_input else []\n",
    "            for dim in dims:\n",
    "                module_list.extend([\n",
    "                    nn.Conv1d(input_dim, dim, 1),\n",
    "                    nn.BatchNorm1d(dim),\n",
    "                    nn.GELU() if activation == 'gelu' else nn.ReLU(),\n",
    "                ])\n",
    "                input_dim = dim\n",
    "            if use_pre_activation_pair:\n",
    "                module_list = module_list[:-1]\n",
    "            self.embed = nn.Sequential(*module_list)\n",
    "        elif self.mode == 'sum':\n",
    "            if pairwise_lv_dim > 0:\n",
    "                input_dim = pairwise_lv_dim\n",
    "                module_list = [nn.BatchNorm1d(input_dim)] if normalize_input else []\n",
    "                for dim in dims:\n",
    "                    module_list.extend([\n",
    "                        nn.Conv1d(input_dim, dim, 1),\n",
    "                        nn.BatchNorm1d(dim),\n",
    "                        nn.GELU() if activation == 'gelu' else nn.ReLU(),\n",
    "                    ])\n",
    "                    input_dim = dim\n",
    "                if use_pre_activation_pair:\n",
    "                    module_list = module_list[:-1]\n",
    "                self.embed = nn.Sequential(*module_list)\n",
    "\n",
    "            if pairwise_input_dim > 0:\n",
    "                input_dim = pairwise_input_dim\n",
    "                module_list = [nn.BatchNorm1d(input_dim)] if normalize_input else []\n",
    "                for dim in dims:\n",
    "                    module_list.extend([\n",
    "                        nn.Conv1d(input_dim, dim, 1),\n",
    "                        nn.BatchNorm1d(dim),\n",
    "                        nn.GELU() if activation == 'gelu' else nn.ReLU(),\n",
    "                    ])\n",
    "                    input_dim = dim\n",
    "                if use_pre_activation_pair:\n",
    "                    module_list = module_list[:-1]\n",
    "                self.fts_embed = nn.Sequential(*module_list)\n",
    "        else:\n",
    "            raise RuntimeError('`mode` can only be `sum` or `concat`')\n",
    "\n",
    "    def forward(self, x, uu=None):\n",
    "        # x: (batch, v_dim, seq_len)\n",
    "        # uu: (batch, v_dim, seq_len, seq_len)\n",
    "        assert (x is not None or uu is not None)\n",
    "        with torch.no_grad():\n",
    "            if x is not None:\n",
    "                batch_size, _, seq_len = x.size()\n",
    "            else:\n",
    "                batch_size, _, seq_len, _ = uu.size()\n",
    "            if self.is_symmetric and not self.for_onnx:\n",
    "                i, j = torch.tril_indices(seq_len, seq_len, offset=-1 if self.remove_self_pair else 0,\n",
    "                                          device=(x if x is not None else uu).device)\n",
    "                if x is not None:\n",
    "                    x = x.unsqueeze(-1).repeat(1, 1, 1, seq_len)\n",
    "                    xi = x[:, :, i, j]  # (batch, dim, seq_len*(seq_len+1)/2)\n",
    "                    xj = x[:, :, j, i]\n",
    "                    x = self.pairwise_lv_fts(xi, xj)\n",
    "                if uu is not None:\n",
    "                    # (batch, dim, seq_len*(seq_len+1)/2)\n",
    "                    uu = uu[:, :, i, j]\n",
    "            else:\n",
    "                if x is not None:\n",
    "                    x = self.pairwise_lv_fts(x.unsqueeze(-1), x.unsqueeze(-2))\n",
    "                    if self.remove_self_pair:\n",
    "                        i = torch.arange(0, seq_len, device=x.device)\n",
    "                        x[:, :, i, i] = 0\n",
    "                    x = x.view(-1, self.pairwise_lv_dim, seq_len * seq_len)\n",
    "                if uu is not None:\n",
    "                    uu = uu.view(-1, self.pairwise_input_dim, seq_len * seq_len)\n",
    "            if self.mode == 'concat':\n",
    "                if x is None:\n",
    "                    pair_fts = uu\n",
    "                elif uu is None:\n",
    "                    pair_fts = x\n",
    "                else:\n",
    "                    pair_fts = torch.cat((x, uu), dim=1)\n",
    "\n",
    "        if self.mode == 'concat':\n",
    "            elements = self.embed(pair_fts)  # (batch, embed_dim, num_elements)\n",
    "        elif self.mode == 'sum':\n",
    "            if x is None:\n",
    "                elements = self.fts_embed(uu)\n",
    "            elif uu is None:\n",
    "                elements = self.embed(x)\n",
    "            else:\n",
    "                elements = self.embed(x) + self.fts_embed(uu)\n",
    "\n",
    "        if self.is_symmetric and not self.for_onnx:\n",
    "            y = torch.zeros(batch_size, self.out_dim, seq_len, seq_len, dtype=elements.dtype, device=elements.device)\n",
    "            y[:, :, i, j] = elements\n",
    "            y[:, :, j, i] = elements\n",
    "        else:\n",
    "            y = elements.view(-1, self.out_dim, seq_len, seq_len)\n",
    "        return y\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, embed_dim=128, num_heads=8, ffn_ratio=4,\n",
    "                 dropout=0.1, attn_dropout=0.1, activation_dropout=0.1,\n",
    "                 add_bias_kv=False, activation='gelu',\n",
    "                 scale_fc=True, scale_attn=True, scale_heads=True, scale_resids=True,\n",
    "                 return_pre_softmax=False, gate_low_rank_dim=4):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.ffn_dim = embed_dim * ffn_ratio\n",
    "        self.interaction = None\n",
    "        self.gate_low_rank_dim = gate_low_rank_dim\n",
    "        self.pre_mask_attn_weights = None  # To store attention weights before mask is applied\n",
    "        self.return_pre_softmax = return_pre_softmax\n",
    "\n",
    "        self.pre_attn_norm = nn.LayerNorm(embed_dim)\n",
    "        self.attn = GatedLinearAttention(\n",
    "            hidden_size=embed_dim,\n",
    "            num_heads=num_heads,\n",
    "            gate_low_rank_dim=gate_low_rank_dim,\n",
    "        )\n",
    "        self.post_attn_norm = nn.LayerNorm(embed_dim) if scale_attn else None\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.pre_fc_norm = nn.LayerNorm(embed_dim)\n",
    "        self.fc1 = nn.Linear(embed_dim, self.ffn_dim)\n",
    "        self.act = nn.GELU() if activation == 'gelu' else nn.ReLU()\n",
    "        self.act_dropout = nn.Dropout(activation_dropout)\n",
    "        self.post_fc_norm = nn.LayerNorm(self.ffn_dim) if scale_fc else None\n",
    "        self.fc2 = nn.Linear(self.ffn_dim, embed_dim)\n",
    "\n",
    "        self.c_attn = nn.Parameter(torch.ones(num_heads), requires_grad=True) if scale_heads else None\n",
    "        self.w_resid = nn.Parameter(torch.ones(embed_dim), requires_grad=True) if scale_resids else None\n",
    "    def getAttention(self):\n",
    "        return self.interaction\n",
    "    def getPreMaskAttention(self):\n",
    "        return self.pre_mask_attn_weights\n",
    "    def forward(self, x, x_cls=None, padding_mask=None, attn_mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (Tensor): input to the layer of shape (seq_len, batch, embed_dim)\n",
    "            x_cls (Tensor, optional): class token input to the layer of shape (1, batch, embed_dim)\n",
    "            padding_mask (ByteTensor, optional): binary\n",
    "                ByteTensor of shape (batch, seq_len) where padding\n",
    "                elements are indicated by `1.\n",
    "\n",
    "        Returns:\n",
    "            encoded output of shape (seq_len, batch, embed_dim)\n",
    "        \"\"\"\n",
    "\n",
    "        if x_cls is not None:\n",
    "            with torch.no_grad():\n",
    "                # prepend one element for x_cls: -> (batch, 1+seq_len)\n",
    "                padding_mask = torch.cat((torch.zeros_like(padding_mask[:, :1]), padding_mask), dim=1)\n",
    "            # class attention: https://arxiv.org/pdf/2103.17239.pdf\n",
    "            residual = x_cls\n",
    "            assert isinstance(x_cls, Tensor), \"x_cls should be a Tensor\"\n",
    "            assert isinstance(x, Tensor), \"x should be a Tensor\"\n",
    "            u = torch.cat(tensors=(x_cls, x), dim=0)  # (seq_len+1, batch, embed_dim)\n",
    "            u = self.pre_attn_norm(u)\n",
    "\n",
    "            if self.return_pre_softmax:\n",
    "                x, _, pre_softmax_attention, pre_softmax_interaction = self.attn(\n",
    "                    hidden_states=x_cls, k=u, v=u, attention_mask=padding_mask,\n",
    "                    )\n",
    "                #pre_softmax_attention.cpu().detach()\n",
    "                #pre_softmax_interaction.cpu().detach()\n",
    "            else:\n",
    "\n",
    "                x = self.attn(hidden_states=x_cls, k=u, v=u, attention_mask=padding_mask)  # (1, batch, embed_dim)\n",
    "\n",
    "            pre_softmax_attention = None\n",
    "            pre_softmax_interaction = None\n",
    "        else:\n",
    "            residual = x\n",
    "\n",
    "            x = self.pre_attn_norm(x)\n",
    "\n",
    "            if self.return_pre_softmax:\n",
    "                x, y, pre_softmax_attention, pre_softmax_interaction = self.attn(\n",
    "                    x, x, x, key_padding_mask=padding_mask,\n",
    "                    attn_mask=attn_mask, average_attn_weights=False, return_pre_softmax=self.return_pre_softmax)\n",
    "                #y = self.attn(x, x, x, key_padding_mask=padding_mask,\n",
    "                #    attn_mask=attn_mask, average_attn_weights=False, return_pre_softmax=self.return_pre_softmax)[1]\n",
    "                #pre_softmax_attention = self.attn(x, x, x, key_padding_mask=padding_mask, \n",
    "                #    attn_mask=attn_mask, average_attn_weights=False, return_pre_softmax=self.return_pre_softmax)[2]\n",
    "                #pre_softmax_interaction = self.attn(x, x, x, key_padding_mask=padding_mask,\n",
    "                #    attn_mask=attn_mask, average_attn_weights=False, return_pre_softmax=self.return_pre_softmax)[3]\n",
    "                #pre_softmax_attention.cpu().detach()\n",
    "                #pre_softmax_interaction.cpu().detach()\n",
    "            \n",
    "            else:\n",
    "                x = self.attn(\n",
    "                    x, x, x, attention_mask=padding_mask)  # (seq_len, batch, embed_dim)\n",
    "                y = self.attn(\n",
    "                    x, x, x, key_padding_mask=padding_mask,\n",
    "                    attn_mask=attn_mask, average_attn_weights=False, return_pre_softmax=self.return_pre_softmax)[1]\n",
    "            \n",
    "            self.interaction = y\n",
    "\n",
    "\n",
    "        if self.c_attn is not None:\n",
    "            tgt_len = x.size(0)\n",
    "            x = x.view(tgt_len, -1, self.num_heads, self.head_dim)\n",
    "            x = torch.einsum('tbhd,h->tbdh', x, self.c_attn)\n",
    "            x = x.reshape(tgt_len, -1, self.embed_dim)\n",
    "        if self.post_attn_norm is not None:\n",
    "            x = self.post_attn_norm(x)\n",
    "        x = self.dropout(x)\n",
    "        x += residual\n",
    "\n",
    "        residual = x\n",
    "        x = self.pre_fc_norm(x)\n",
    "        x = self.act(self.fc1(x))\n",
    "        x = self.act_dropout(x)\n",
    "        if self.post_fc_norm is not None:\n",
    "            x = self.post_fc_norm(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.dropout(x)\n",
    "        if self.w_resid is not None:\n",
    "            residual = torch.mul(self.w_resid, residual)\n",
    "        x += residual\n",
    "\n",
    "        if self.return_pre_softmax:\n",
    "            return x, pre_softmax_attention, pre_softmax_interaction\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "\n",
    "class ParticleTransformer(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 input_dim,\n",
    "                 num_classes=10,\n",
    "                 # network configurations\n",
    "                 pair_input_dim=4,\n",
    "                 pair_extra_dim=0,\n",
    "                 remove_self_pair=False,\n",
    "                 use_pre_activation_pair=True,\n",
    "                 embed_dims=[64, 64, 64],\n",
    "                 pair_embed_dims=[32, 32, 32],\n",
    "                 num_heads=1,\n",
    "                 num_layers=1,\n",
    "                 num_cls_layers=1,\n",
    "                 block_params=None,\n",
    "                 cls_block_params={'dropout': 0, 'attn_dropout': 0, 'activation_dropout': 0},\n",
    "                 fc_params=[],\n",
    "                 activation='gelu',\n",
    "                 # misc\n",
    "                 trim=True,\n",
    "                 for_inference=False,\n",
    "                 use_amp=False,\n",
    "                 return_pre_softmax=False,\n",
    "                 **kwargs) -> None:\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.trimmer = SequenceTrimmer(enabled=trim and not for_inference)\n",
    "        self.attention_matrix = []\n",
    "        self.for_inference = for_inference\n",
    "        self.use_amp = use_amp\n",
    "        self.return_pre_softmax = return_pre_softmax\n",
    "\n",
    "        embed_dim = embed_dims[-1] if len(embed_dims) > 0 else input_dim\n",
    "        self.default_cfg = default_cfg = dict(embed_dim=embed_dim, num_heads=num_heads, \n",
    "                                            gate_low_rank_dim=4)\n",
    "        self.pairMatrixes = []\n",
    "\n",
    "        cfg_block = copy.deepcopy(default_cfg)\n",
    "        if block_params is not None:\n",
    "            cfg_block.update(block_params)\n",
    "        _logger.info('cfg_block: %s' % str(cfg_block))\n",
    "\n",
    "        cfg_cls_block = copy.deepcopy(default_cfg)\n",
    "        if cls_block_params is not None:\n",
    "            cfg_cls_block.update(cls_block_params)\n",
    "        _logger.info('cfg_cls_block: %s' % str(cfg_cls_block))\n",
    "\n",
    "        self.pair_extra_dim = pair_extra_dim\n",
    "        self.embed = Embed(input_dim, embed_dims, activation=activation) if len(embed_dims) > 0 else nn.Identity()\n",
    "        self.pair_embed = PairEmbed(\n",
    "            pair_input_dim, pair_extra_dim, pair_embed_dims + [cfg_block['num_heads']],\n",
    "            remove_self_pair=remove_self_pair, use_pre_activation_pair=use_pre_activation_pair,\n",
    "            for_onnx=for_inference) if pair_embed_dims is not None and pair_input_dim + pair_extra_dim > 0 else None\n",
    "        self.blocks = nn.ModuleList([Block(**cfg_block) for _ in range(num_layers)])\n",
    "        self.cls_blocks = nn.ModuleList([Block(**cfg_cls_block) for _ in range(num_cls_layers)])\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.interactionMatrix = None\n",
    "\n",
    "        if fc_params is not None:\n",
    "            fcs = []\n",
    "            in_dim = embed_dim\n",
    "            for out_dim, drop_rate in fc_params:\n",
    "                fcs.append(nn.Sequential(nn.Linear(in_dim, out_dim), nn.ReLU(), nn.Dropout(drop_rate)))\n",
    "                in_dim = out_dim\n",
    "            fcs.append(nn.Linear(in_dim, num_classes))\n",
    "            self.fc = nn.Sequential(*fcs)\n",
    "        else:\n",
    "            self.fc = None\n",
    "\n",
    "        # init\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim), requires_grad=True)\n",
    "        trunc_normal_(self.cls_token, std=.02)\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay(self):\n",
    "        return {'cls_token', }\n",
    "\n",
    "    def getAttention(self):\n",
    "        return self.attention_matrix\n",
    "\n",
    "    def getInteraction(self):\n",
    "        return self.interactionMatrix\n",
    "\n",
    "    def getPairs(self):\n",
    "        return self.pairMatrixes\n",
    "\n",
    "\n",
    "    def forward(self, x, v=None, mask=None, uu=None, uu_idx=None):\n",
    "        # x: (N, C, P)\n",
    "        # v: (N, 4, P) [px,py,pz,energy]\n",
    "        # mask: (N, 1, P) -- real particle = 1, padded = 0\n",
    "        # for pytorch: uu (N, C', num_pairs), uu_idx (N, 2, num_pairs)\n",
    "        # for onnx: uu (N, C', P, P), uu_idx=None\n",
    "\n",
    "        with torch.no_grad():\n",
    "            if not self.for_inference:\n",
    "                if uu_idx is not None:\n",
    "                    uu = build_sparse_tensor(uu, uu_idx, x.size(-1))\n",
    "            x, v, mask, uu = self.trimmer(x, v, mask, uu)\n",
    "            padding_mask = ~mask.squeeze(1)  # (N, P)\n",
    "\n",
    "        with torch.amp.autocast('cuda',enabled=self.use_amp):\n",
    "            # input embedding\n",
    "            x = self.embed(x).masked_fill(~mask.permute(2, 0, 1), 0)  # (P, N, C)\n",
    "            attn_mask = None\n",
    "            #for now, we can run inference without pair interaction features\n",
    "            #if (v is not None or uu is not None) and self.pair_embed is not None:\n",
    "            #    attn_mask = self.pair_embed(v, uu).view(-1, v.size(-1), v.size(-1))  # (N*num_heads, P, P)\n",
    "\n",
    "            # transform\n",
    "            #num = 0\n",
    "            for block in self.blocks:\n",
    "                if self.return_pre_softmax:\n",
    "                    x = block(x, x_cls=None, padding_mask=padding_mask, attn_mask=attn_mask)[0]\n",
    "                else:\n",
    "                    x = block(x, x_cls=None, padding_mask=padding_mask, attn_mask=attn_mask)\n",
    "                self.interactionMatrix = attn_mask\n",
    "                #if num == 0 :\n",
    "                self.attention_matrix.append(block.interaction)\n",
    "                #num = num + 1\n",
    "\n",
    "            # extract class token\n",
    "            cls_tokens = self.cls_token.expand(1, x.size(1), -1)  # (1, N, C)\n",
    "            for block in self.cls_blocks:\n",
    "                if self.return_pre_softmax:\n",
    "                    cls_tokens = block(x, x_cls=cls_tokens, padding_mask=padding_mask)[0]\n",
    "                else:\n",
    "                    cls_tokens = block(x, x_cls=cls_tokens, padding_mask=padding_mask)\n",
    "\n",
    "            x_cls = self.norm(cls_tokens).squeeze(0)\n",
    "\n",
    "            # fc\n",
    "            if self.fc is None:\n",
    "                return x_cls\n",
    "            output = self.fc(x_cls)\n",
    "            if self.for_inference:\n",
    "                output = torch.softmax(output, dim=1)\n",
    "\n",
    "            return output\n",
    "\n",
    "class ParticleTransformerTagger(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 pf_input_dim,\n",
    "                 sv_input_dim,\n",
    "                 num_classes=None,\n",
    "                 # network configurations\n",
    "                 pair_input_dim=4,\n",
    "                 pair_extra_dim=0,\n",
    "                 remove_self_pair=False,\n",
    "                 use_pre_activation_pair=True,\n",
    "                 embed_dims=[128, 512, 128],\n",
    "                 pair_embed_dims=[64, 64, 64],\n",
    "                 num_heads=8,\n",
    "                 num_layers=8,\n",
    "                 num_cls_layers=2,\n",
    "                 block_params=None,\n",
    "                 cls_block_params={'dropout': 0, 'attn_dropout': 0, 'activation_dropout': 0},\n",
    "                 fc_params=[],\n",
    "                 activation='gelu',\n",
    "                 # misc\n",
    "                 trim=True,\n",
    "                 for_inference=False,\n",
    "                 use_amp=False,\n",
    "                 **kwargs) -> None:\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.use_amp = use_amp\n",
    "\n",
    "        self.pf_trimmer = SequenceTrimmer(enabled=trim and not for_inference)\n",
    "        self.sv_trimmer = SequenceTrimmer(enabled=trim and not for_inference)\n",
    "\n",
    "        self.pf_embed = Embed(pf_input_dim, embed_dims, activation=activation)\n",
    "        self.sv_embed = Embed(sv_input_dim, embed_dims, activation=activation)\n",
    "\n",
    "        self.part = ParticleTransformer(input_dim=embed_dims[-1],\n",
    "                                        num_classes=num_classes,\n",
    "                                        # network configurations\n",
    "                                        pair_input_dim=pair_input_dim,\n",
    "                                        pair_extra_dim=pair_extra_dim,\n",
    "                                        remove_self_pair=remove_self_pair,\n",
    "                                        use_pre_activation_pair=use_pre_activation_pair,\n",
    "                                        embed_dims=[],\n",
    "                                        pair_embed_dims=pair_embed_dims,\n",
    "                                        num_heads=num_heads,\n",
    "                                        num_layers=num_layers,\n",
    "                                        num_cls_layers=num_cls_layers,\n",
    "                                        block_params=block_params,\n",
    "                                        cls_block_params=cls_block_params,\n",
    "                                        fc_params=fc_params,\n",
    "                                        activation=activation,\n",
    "                                        # misc\n",
    "                                        trim=False,\n",
    "                                        for_inference=for_inference,\n",
    "                                        use_amp=use_amp)\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay(self):\n",
    "        return {'part.cls_token', }\n",
    "\n",
    "    def forward(self, pf_x, pf_v=None, pf_mask=None, sv_x=None, sv_v=None, sv_mask=None):\n",
    "        # x: (N, C, P)\n",
    "        # v: (N, 4, P) [px,py,pz,energy]\n",
    "        # mask: (N, 1, P) -- real particle = 1, padded = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            pf_x, pf_v, pf_mask, _ = self.pf_trimmer(pf_x, pf_v, pf_mask)\n",
    "            sv_x, sv_v, sv_mask, _ = self.sv_trimmer(sv_x, sv_v, sv_mask)\n",
    "            v = torch.cat([pf_v, sv_v], dim=2)\n",
    "            mask = torch.cat([pf_mask, sv_mask], dim=2)\n",
    "\n",
    "        with torch.amp.autocast('cuda', enabled=self.use_amp):\n",
    "            pf_x = self.pf_embed(pf_x)  # after embed: (seq_len, batch, embed_dim)\n",
    "            sv_x = self.sv_embed(sv_x)\n",
    "            x = torch.cat([pf_x, sv_x], dim=0)\n",
    "\n",
    "            return self.part(x, v, mask)\n",
    "\n",
    "\n",
    "class ParticleTransformerTaggerWithExtraPairFeatures(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 pf_input_dim,\n",
    "                 sv_input_dim,\n",
    "                 num_classes=None,\n",
    "                 # network configurations\n",
    "                 pair_input_dim=4,\n",
    "                 pair_extra_dim=0,\n",
    "                 remove_self_pair=False,\n",
    "                 use_pre_activation_pair=True,\n",
    "                 embed_dims=[128, 512, 128],\n",
    "                 pair_embed_dims=[64, 64, 64],\n",
    "                 num_heads=8,\n",
    "                 num_layers=8,\n",
    "                 num_cls_layers=2,\n",
    "                 block_params=None,\n",
    "                 cls_block_params={'dropout': 0, 'attn_dropout': 0, 'activation_dropout': 0},\n",
    "                 fc_params=[],\n",
    "                 activation='gelu',\n",
    "                 # misc\n",
    "                 trim=True,\n",
    "                 for_inference=False,\n",
    "                 use_amp=False,\n",
    "                 **kwargs) -> None:\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.use_amp = use_amp\n",
    "        self.for_inference = for_inference\n",
    "\n",
    "        self.pf_trimmer = SequenceTrimmer(enabled=trim and not for_inference)\n",
    "        self.sv_trimmer = SequenceTrimmer(enabled=trim and not for_inference)\n",
    "\n",
    "        self.pf_embed = Embed(pf_input_dim, embed_dims, activation=activation)\n",
    "        self.sv_embed = Embed(sv_input_dim, embed_dims, activation=activation)\n",
    "\n",
    "        self.part = ParticleTransformer(input_dim=embed_dims[-1],\n",
    "                                        num_classes=num_classes,\n",
    "                                        # network configurations\n",
    "                                        pair_input_dim=pair_input_dim,\n",
    "                                        pair_extra_dim=pair_extra_dim,\n",
    "                                        remove_self_pair=remove_self_pair,\n",
    "                                        use_pre_activation_pair=use_pre_activation_pair,\n",
    "                                        embed_dims=[],\n",
    "                                        pair_embed_dims=pair_embed_dims,\n",
    "                                        num_heads=num_heads,\n",
    "                                        num_layers=num_layers,\n",
    "                                        num_cls_layers=num_cls_layers,\n",
    "                                        block_params=block_params,\n",
    "                                        cls_block_params=cls_block_params,\n",
    "                                        fc_params=fc_params,\n",
    "                                        activation=activation,\n",
    "                                        # misc\n",
    "                                        trim=False,\n",
    "                                        for_inference=for_inference,\n",
    "                                        use_amp=use_amp)\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay(self):\n",
    "        return {'part.cls_token', }\n",
    "\n",
    "    def forward(self, pf_x, pf_v=None, pf_mask=None, sv_x=None, sv_v=None, sv_mask=None, pf_uu=None, pf_uu_idx=None):\n",
    "        # x: (N, C, P)\n",
    "        # v: (N, 4, P) [px,py,pz,energy]\n",
    "        # mask: (N, 1, P) -- real particle = 1, padded = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            if not self.for_inference:\n",
    "                if pf_uu_idx is not None:\n",
    "                    pf_uu = build_sparse_tensor(pf_uu, pf_uu_idx, pf_x.size(-1))\n",
    "\n",
    "            pf_x, pf_v, pf_mask, pf_uu = self.pf_trimmer(pf_x, pf_v, pf_mask, pf_uu)\n",
    "            sv_x, sv_v, sv_mask, _ = self.sv_trimmer(sv_x, sv_v, sv_mask)\n",
    "            v = torch.cat([pf_v, sv_v], dim=2)\n",
    "            mask = torch.cat([pf_mask, sv_mask], dim=2)\n",
    "            uu = torch.zeros(v.size(0), pf_uu.size(1), v.size(2), v.size(2), dtype=v.dtype, device=v.device)\n",
    "            uu[:, :, :pf_x.size(2), :pf_x.size(2)] = pf_uu\n",
    "\n",
    "        with torch.amp.autocast('cuda',enabled=self.use_amp):\n",
    "            pf_x = self.pf_embed(pf_x)  # after embed: (seq_len, batch, embed_dim)\n",
    "            sv_x = self.sv_embed(sv_x)\n",
    "            x = torch.cat([pf_x, sv_x], dim=0)\n",
    "\n",
    "            return self.part(x, v, mask, uu)\n",
    "\n",
    "class ParticleTransformerWrapper(torch.nn.Module):\n",
    "    def __init__(self, **kwargs) -> None:\n",
    "        super().__init__()\n",
    "        self.mod = ParticleTransformer(**kwargs)\n",
    "        self.kwargs = kwargs\n",
    "        self.attention_matrix = None\n",
    "        self.interactionMatrix = None\n",
    "        self.pre_mask_attention_matrices = []\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay(self):\n",
    "        return {'mod.cls_token', }\n",
    "\n",
    "    def forward(self, points, features, lorentz_vectors, mask):\n",
    "        output = self.mod(features, v=lorentz_vectors, mask=mask)\n",
    "        self.attention_matrix = self.mod.getAttention()\n",
    "        self.interactionMatrix = self.mod.getInteraction()\n",
    "        #self.pre_mask_attention_matrices = self.get_pre_mask_attention_matrices()\n",
    "        return output\n",
    "\n",
    "    def get_attention_matrix(self):\n",
    "        return self.attention_matrix\n",
    "\n",
    "    def get_interactionMatrix(self):\n",
    "\n",
    "        return self.interactionMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ac6bb199",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(model_type='qg',**kwargs):\n",
    "\n",
    "    if model_type == 'qg':\n",
    "        # QuarkGluon model configuration (13 kinpid features)\n",
    "        cfg = dict(\n",
    "            input_dim=13,  # pt_log, e_log, logptrel, logerel, deltaR, charge, isChargedHadron, isNeutralHadron, isPhoton, isElectron, isMuon, deta, dphi\n",
    "            num_classes=2,  # Quark vs Gluon\n",
    "            pair_input_dim=4,\n",
    "            use_pre_activation_pair=False,\n",
    "            embed_dims=[128, 512, 128],\n",
    "            pair_embed_dims=[64, 64, 64],\n",
    "            num_heads=8,\n",
    "            num_layers=8,\n",
    "            num_cls_layers=2,\n",
    "            block_params=None,\n",
    "            cls_block_params={'dropout': 0, 'attn_dropout': 0, 'activation_dropout': 0},\n",
    "            fc_params=[],\n",
    "            activation='gelu',\n",
    "            trim=True,\n",
    "            for_inference=False,\n",
    "        )\n",
    "    elif model_type == 'tl':\n",
    "        # TopLandscape model configuration (7 kinematic features)\n",
    "        cfg = dict(\n",
    "            input_dim=7,  # part_pt_log, part_e_log, part_logptrel, part_logerel, part_deltaR, part_deta, part_dphi\n",
    "            num_classes=2,  # Top vs QCD\n",
    "            pair_input_dim=4,\n",
    "            use_pre_activation_pair=False,\n",
    "            embed_dims=[128, 512, 128],\n",
    "            pair_embed_dims=[64, 64, 64],\n",
    "            num_heads=8,\n",
    "            num_layers=8,\n",
    "            num_cls_layers=2,\n",
    "            block_params=None,\n",
    "            cls_block_params={'dropout': 0, 'attn_dropout': 0, 'activation_dropout': 0},\n",
    "            fc_params=[],\n",
    "            activation='gelu',\n",
    "            trim=True,\n",
    "            for_inference=False,\n",
    "        )\n",
    "    elif model_type == 'jck':\n",
    "        # JetClass kin model configuration (7 kinematic features)\n",
    "        cfg = dict(\n",
    "            input_dim=7,  # part_pt_log, part_e_log, part_logptrel, part_logerel, part_deltaR, part_deta, part_dphi\n",
    "            num_classes=10,\n",
    "            pair_input_dim=4,\n",
    "            use_pre_activation_pair=False,\n",
    "            embed_dims=[128, 512, 128],\n",
    "            pair_embed_dims=[64, 64, 64],\n",
    "            num_heads=8,\n",
    "            num_layers=8,\n",
    "            num_cls_layers=2,\n",
    "            block_params=None,\n",
    "            cls_block_params={'dropout': 0, 'attn_dropout': 0, 'activation_dropout': 0},\n",
    "            fc_params=[],\n",
    "            activation='gelu',\n",
    "            trim=True,\n",
    "            for_inference=False,\n",
    "        )\n",
    "    elif model_type == 'jck_pid':\n",
    "        # JetClass kin model configuration (7 kinematic features)\n",
    "        cfg = dict(\n",
    "            input_dim=13,  # part_pt_log, part_e_log, part_logptrel, part_logerel, part_deltaR, part_deta, part_dphi along with PID features\n",
    "            num_classes=10,\n",
    "            pair_input_dim=4,\n",
    "            use_pre_activation_pair=False,\n",
    "            embed_dims=[128, 512, 128],\n",
    "            pair_embed_dims=[64, 64, 64],\n",
    "            num_heads=8,\n",
    "            num_layers=8,\n",
    "            num_cls_layers=2,\n",
    "            block_params=None,\n",
    "            cls_block_params={'dropout': 0, 'attn_dropout': 0, 'activation_dropout': 0},\n",
    "            fc_params=[],\n",
    "            activation='gelu',\n",
    "            trim=True,\n",
    "            for_inference=False,\n",
    "        )\n",
    "    elif model_type == 'jck_binary':\n",
    "        cfg = dict(\n",
    "            input_dim=7,  # part_pt_log, part_e_log, part_logptrel, part_logerel, part_deltaR, part_deta, part_dphi along with PID features\n",
    "            num_classes=2,\n",
    "            pair_input_dim=4,\n",
    "            use_pre_activation_pair=False,\n",
    "            embed_dims=[128, 512, 128],\n",
    "            pair_embed_dims=[64, 64, 64],\n",
    "            num_heads=8,\n",
    "            num_layers=8,\n",
    "            num_cls_layers=2,\n",
    "            block_params=None,\n",
    "            cls_block_params={'dropout': 0, 'attn_dropout': 0, 'activation_dropout': 0},\n",
    "            fc_params=[],\n",
    "            activation='gelu',\n",
    "            trim=True,\n",
    "            for_inference=False,\n",
    "            return_pre_softmax=True,\n",
    "        )\n",
    "    elif model_type == 'jc_full_binary':\n",
    "        cfg = dict(\n",
    "            input_dim=17,  # part_pt_log, part_e_log, part_logptrel, part_logerel, part_deltaR, part_deta, part_dphi along with PID features\n",
    "            num_classes=2,\n",
    "            pair_input_dim=4,\n",
    "            use_pre_activation_pair=False,\n",
    "            embed_dims=[128, 512, 128],\n",
    "            pair_embed_dims=[64, 64, 64],\n",
    "            num_heads=8,\n",
    "            num_layers=8,\n",
    "            num_cls_layers=2,\n",
    "            block_params=None,\n",
    "            cls_block_params={'dropout': 0, 'attn_dropout': 0, 'activation_dropout': 0},\n",
    "            fc_params=[],\n",
    "            activation='gelu',\n",
    "            trim=True,\n",
    "            for_inference=False,\n",
    "            return_pre_softmax=True,\n",
    "        )\n",
    "    else:\n",
    "        print(f\"Defaulting to Jet_Class-trained model configuration\")\n",
    "        cfg = dict(\n",
    "            input_dim=17,\n",
    "            num_classes=10,\n",
    "            # network configurations\n",
    "            pair_input_dim=4,\n",
    "            use_pre_activation_pair=False,\n",
    "            embed_dims=[128, 512, 128],\n",
    "            pair_embed_dims=[64, 64, 64],\n",
    "            num_heads=8,\n",
    "            num_layers=8,\n",
    "            num_cls_layers=2,\n",
    "            block_params=None,\n",
    "            cls_block_params={'dropout': 0, 'attn_dropout': 0, 'activation_dropout': 0},\n",
    "            fc_params=[],\n",
    "            activation='gelu',\n",
    "            # misc\n",
    "            trim=True,\n",
    "            for_inference=False,\n",
    "        )\n",
    "    \n",
    "    cfg.update(**kwargs)\n",
    "    model = ParticleTransformerWrapper(**cfg)\n",
    "\n",
    "    model_info = {\n",
    "\n",
    "    }\n",
    "\n",
    "    return model, model_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ae1f4fe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to Jet_Class-trained model configuration\n"
     ]
    }
   ],
   "source": [
    "model, _ = get_model('jc_full')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8916c179",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 17, 128)\n",
      "(10, 10)\n",
      "(10, 1, 128)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 10\n",
    "\n",
    "pf_features = np.load('./jc_full_data_new/jc_full_pf_features.npy')[:batch_size]\n",
    "pf_vectors = np.load('./jc_full_data_new/jc_full_pf_vectors.npy')[:batch_size]\n",
    "pf_mask = np.load('./jc_full_data_new/jc_full_pf_mask.npy')[:batch_size]\n",
    "pf_points = np.load('./jc_full_data_new/jc_full_pf_points.npy')[:batch_size]\n",
    "labels = np.load('./jc_full_data_new/jc_full_labels.npy')[:batch_size]\n",
    "\n",
    "print(pf_features.shape)\n",
    "print(labels.shape)\n",
    "print(pf_mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1aef8418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[1. 1. 1. ... 0. 0. 0.]]\n",
      "\n",
      " [[1. 1. 1. ... 0. 0. 0.]]\n",
      "\n",
      " [[1. 1. 1. ... 0. 0. 0.]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[1. 1. 1. ... 0. 0. 0.]]\n",
      "\n",
      " [[1. 1. 1. ... 0. 0. 0.]]\n",
      "\n",
      " [[1. 1. 1. ... 0. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "print(pf_mask[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f07115d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape pf_mask into (batch_size, seq_len)\n",
    "\n",
    "#pf_mask = np.reshape(pf_mask, (20, 128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ccbbfc78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ParticleTransformerWrapper(\n",
      "  (mod): ParticleTransformer(\n",
      "    (trimmer): SequenceTrimmer()\n",
      "    (embed): Embed(\n",
      "      (input_bn): BatchNorm1d(17, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (embed): Sequential(\n",
      "        (0): LayerNorm((17,), eps=1e-05, elementwise_affine=True)\n",
      "        (1): Linear(in_features=17, out_features=128, bias=True)\n",
      "        (2): GELU(approximate='none')\n",
      "        (3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (4): Linear(in_features=128, out_features=512, bias=True)\n",
      "        (5): GELU(approximate='none')\n",
      "        (6): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (7): Linear(in_features=512, out_features=128, bias=True)\n",
      "        (8): GELU(approximate='none')\n",
      "      )\n",
      "    )\n",
      "    (pair_embed): PairEmbed(\n",
      "      (embed): Sequential(\n",
      "        (0): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (1): Conv1d(4, 64, kernel_size=(1,), stride=(1,))\n",
      "        (2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (3): GELU(approximate='none')\n",
      "        (4): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
      "        (5): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (6): GELU(approximate='none')\n",
      "        (7): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
      "        (8): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (9): GELU(approximate='none')\n",
      "        (10): Conv1d(64, 8, kernel_size=(1,), stride=(1,))\n",
      "        (11): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (12): GELU(approximate='none')\n",
      "      )\n",
      "    )\n",
      "    (blocks): ModuleList(\n",
      "      (0-7): 8 x Block(\n",
      "        (pre_attn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GatedLinearAttention(\n",
      "          (q_proj): Linear(in_features=128, out_features=64, bias=False)\n",
      "          (k_proj): Linear(in_features=128, out_features=64, bias=False)\n",
      "          (v_proj): Linear(in_features=128, out_features=128, bias=False)\n",
      "          (g_proj): Linear(in_features=128, out_features=128, bias=False)\n",
      "          (gk_proj): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=4, bias=False)\n",
      "            (1): Linear(in_features=4, out_features=64, bias=True)\n",
      "          )\n",
      "          (o_proj): Linear(in_features=128, out_features=128, bias=False)\n",
      "          (g_norm_swish_gate): FusedRMSNormGated(16, eps=1e-05, activation=swish)\n",
      "        )\n",
      "        (post_attn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (pre_fc_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (act_dropout): Dropout(p=0.1, inplace=False)\n",
      "        (post_fc_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (cls_blocks): ModuleList(\n",
      "      (0-1): 2 x Block(\n",
      "        (pre_attn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GatedLinearAttention(\n",
      "          (q_proj): Linear(in_features=128, out_features=64, bias=False)\n",
      "          (k_proj): Linear(in_features=128, out_features=64, bias=False)\n",
      "          (v_proj): Linear(in_features=128, out_features=128, bias=False)\n",
      "          (g_proj): Linear(in_features=128, out_features=128, bias=False)\n",
      "          (gk_proj): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=4, bias=False)\n",
      "            (1): Linear(in_features=4, out_features=64, bias=True)\n",
      "          )\n",
      "          (o_proj): Linear(in_features=128, out_features=128, bias=False)\n",
      "          (g_norm_swish_gate): FusedRMSNormGated(16, eps=1e-05, activation=swish)\n",
      "        )\n",
      "        (post_attn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0, inplace=False)\n",
      "        (pre_fc_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (act_dropout): Dropout(p=0, inplace=False)\n",
      "        (post_fc_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "    (fc): Sequential(\n",
      "      (0): Linear(in_features=128, out_features=10, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2fe6ddbe",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torch.cpu' has no attribute 'device'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m model.eval()\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpf_points\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpf_features\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpf_vectors\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpf_mask\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m     \u001b[38;5;28mprint\u001b[39m(outputs.shape)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/main_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/main_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 830\u001b[39m, in \u001b[36mParticleTransformerWrapper.forward\u001b[39m\u001b[34m(self, points, features, lorentz_vectors, mask)\u001b[39m\n\u001b[32m    829\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, points, features, lorentz_vectors, mask):\n\u001b[32m--> \u001b[39m\u001b[32m830\u001b[39m     output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmod\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlorentz_vectors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    831\u001b[39m     \u001b[38;5;28mself\u001b[39m.attention_matrix = \u001b[38;5;28mself\u001b[39m.mod.getAttention()\n\u001b[32m    832\u001b[39m     \u001b[38;5;28mself\u001b[39m.interactionMatrix = \u001b[38;5;28mself\u001b[39m.mod.getInteraction()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/main_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/main_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 627\u001b[39m, in \u001b[36mParticleTransformer.forward\u001b[39m\u001b[34m(self, x, v, mask, uu, uu_idx)\u001b[39m\n\u001b[32m    625\u001b[39m     x = block(x, x_cls=\u001b[38;5;28;01mNone\u001b[39;00m, padding_mask=padding_mask, attn_mask=attn_mask)[\u001b[32m0\u001b[39m]\n\u001b[32m    626\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m627\u001b[39m     x = \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpadding_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    628\u001b[39m \u001b[38;5;28mself\u001b[39m.interactionMatrix = attn_mask\n\u001b[32m    629\u001b[39m \u001b[38;5;66;03m#if num == 0 :\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/main_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/main_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 474\u001b[39m, in \u001b[36mBlock.forward\u001b[39m\u001b[34m(self, x, x_cls, padding_mask, attn_mask)\u001b[39m\n\u001b[32m    461\u001b[39m     x, y, pre_softmax_attention, pre_softmax_interaction = \u001b[38;5;28mself\u001b[39m.attn(\n\u001b[32m    462\u001b[39m         x, x, x, key_padding_mask=padding_mask,\n\u001b[32m    463\u001b[39m         attn_mask=attn_mask, average_attn_weights=\u001b[38;5;28;01mFalse\u001b[39;00m, return_pre_softmax=\u001b[38;5;28mself\u001b[39m.return_pre_softmax)\n\u001b[32m    464\u001b[39m     \u001b[38;5;66;03m#y = self.attn(x, x, x, key_padding_mask=padding_mask,\u001b[39;00m\n\u001b[32m    465\u001b[39m     \u001b[38;5;66;03m#    attn_mask=attn_mask, average_attn_weights=False, return_pre_softmax=self.return_pre_softmax)[1]\u001b[39;00m\n\u001b[32m    466\u001b[39m     \u001b[38;5;66;03m#pre_softmax_attention = self.attn(x, x, x, key_padding_mask=padding_mask, \u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    472\u001b[39m \n\u001b[32m    473\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m474\u001b[39m     x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    475\u001b[39m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpadding_mask\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (seq_len, batch, embed_dim)\u001b[39;00m\n\u001b[32m    476\u001b[39m     y = \u001b[38;5;28mself\u001b[39m.attn(\n\u001b[32m    477\u001b[39m         x, x, x, key_padding_mask=padding_mask,\n\u001b[32m    478\u001b[39m         attn_mask=attn_mask, average_attn_weights=\u001b[38;5;28;01mFalse\u001b[39;00m, return_pre_softmax=\u001b[38;5;28mself\u001b[39m.return_pre_softmax)[\u001b[32m1\u001b[39m]\n\u001b[32m    480\u001b[39m \u001b[38;5;28mself\u001b[39m.interaction = y\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/main_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/main_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 263\u001b[39m, in \u001b[36mGatedLinearAttention.forward\u001b[39m\u001b[34m(self, hidden_states, k, v, attention_mask, past_key_values, use_cache, output_attentions, **kwargs)\u001b[39m\n\u001b[32m    261\u001b[39m recurrent_state = last_state[\u001b[33m'\u001b[39m\u001b[33mrecurrent_state\u001b[39m\u001b[33m'\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m last_state \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    262\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mode == \u001b[33m'\u001b[39m\u001b[33mfused_recurrent\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m263\u001b[39m     o, recurrent_state = \u001b[43mfused_recurrent_gla\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    264\u001b[39m \u001b[43m        \u001b[49m\u001b[43mq\u001b[49m\u001b[43m=\u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    265\u001b[39m \u001b[43m        \u001b[49m\u001b[43mk\u001b[49m\u001b[43m=\u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    266\u001b[39m \u001b[43m        \u001b[49m\u001b[43mv\u001b[49m\u001b[43m=\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    267\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgk\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    268\u001b[39m \u001b[43m        \u001b[49m\u001b[43minitial_state\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrecurrent_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    269\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_final_state\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    270\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcu_seqlens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcu_seqlens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    271\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    272\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m mode == \u001b[33m'\u001b[39m\u001b[33mfused_chunk\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m    273\u001b[39m     o, recurrent_state = fused_chunk_gla(\n\u001b[32m    274\u001b[39m         q=q,\n\u001b[32m    275\u001b[39m         k=k,\n\u001b[32m   (...)\u001b[39m\u001b[32m    279\u001b[39m         output_final_state=use_cache,\n\u001b[32m    280\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/main_env/lib/python3.12/site-packages/fla/ops/gla/fused_recurrent.py:98\u001b[39m, in \u001b[36mfused_recurrent_gla\u001b[39m\u001b[34m(q, k, v, gk, gv, scale, initial_state, output_final_state, reverse, cu_seqlens)\u001b[39m\n\u001b[32m     96\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m scale \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     97\u001b[39m     scale = k.shape[-\u001b[32m1\u001b[39m] ** -\u001b[32m0.5\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m98\u001b[39m o, final_state = \u001b[43mfused_recurrent\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     99\u001b[39m \u001b[43m    \u001b[49m\u001b[43mq\u001b[49m\u001b[43m=\u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    100\u001b[39m \u001b[43m    \u001b[49m\u001b[43mk\u001b[49m\u001b[43m=\u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    101\u001b[39m \u001b[43m    \u001b[49m\u001b[43mv\u001b[49m\u001b[43m=\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    102\u001b[39m \u001b[43m    \u001b[49m\u001b[43mg\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    103\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgk\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    104\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgv\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    105\u001b[39m \u001b[43m    \u001b[49m\u001b[43mscale\u001b[49m\u001b[43m=\u001b[49m\u001b[43mscale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    106\u001b[39m \u001b[43m    \u001b[49m\u001b[43minitial_state\u001b[49m\u001b[43m=\u001b[49m\u001b[43minitial_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    107\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_final_state\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_final_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    108\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreverse\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreverse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    109\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcu_seqlens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcu_seqlens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    110\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m o, final_state\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/main_env/lib/python3.12/site-packages/fla/ops/common/fused_recurrent.py:554\u001b[39m, in \u001b[36mfused_recurrent\u001b[39m\u001b[34m(q, k, v, g, g_gamma, gk, gv, scale, initial_state, output_final_state, reverse, cu_seqlens)\u001b[39m\n\u001b[32m    552\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m scale \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    553\u001b[39m     scale = k.shape[-\u001b[32m1\u001b[39m] ** -\u001b[32m0.5\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m554\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mFusedRecurrentFunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    555\u001b[39m \u001b[43m    \u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    556\u001b[39m \u001b[43m    \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    557\u001b[39m \u001b[43m    \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    558\u001b[39m \u001b[43m    \u001b[49m\u001b[43mg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    559\u001b[39m \u001b[43m    \u001b[49m\u001b[43mg_gamma\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    560\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    561\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    562\u001b[39m \u001b[43m    \u001b[49m\u001b[43mscale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    563\u001b[39m \u001b[43m    \u001b[49m\u001b[43minitial_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    564\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_final_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    565\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreverse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    566\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcu_seqlens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    567\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/main_env/lib/python3.12/site-packages/torch/autograd/function.py:576\u001b[39m, in \u001b[36mFunction.apply\u001b[39m\u001b[34m(cls, *args, **kwargs)\u001b[39m\n\u001b[32m    573\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch._C._are_functorch_transforms_active():\n\u001b[32m    574\u001b[39m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[32m    575\u001b[39m     args = _functorch.utils.unwrap_dead_wrappers(args)\n\u001b[32m--> \u001b[39m\u001b[32m576\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m    578\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[32m    579\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    580\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    581\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    582\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mstaticmethod. For more details, please see \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    583\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mhttps://pytorch.org/docs/main/notes/extending.func.html\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    584\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/main_env/lib/python3.12/site-packages/fla/utils.py:154\u001b[39m, in \u001b[36minput_guard.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    151\u001b[39m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    153\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tensor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m154\u001b[39m     ctx = \u001b[43mcustom_device_ctx\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m.\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    155\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    156\u001b[39m     ctx = contextlib.nullcontext()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/main_env/lib/python3.12/site-packages/fla/utils.py:455\u001b[39m, in \u001b[36mcustom_device_ctx\u001b[39m\u001b[34m(index)\u001b[39m\n\u001b[32m    454\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcustom_device_ctx\u001b[39m(index: \u001b[38;5;28mint\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m455\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdevice_torch_lib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m(index)\n",
      "\u001b[31mAttributeError\u001b[39m: module 'torch.cpu' has no attribute 'device'"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(torch.from_numpy(pf_points).float(),\n",
    "                        torch.from_numpy(pf_features).float(), \n",
    "                        torch.from_numpy(pf_vectors).float(), \n",
    "                        torch.from_numpy(pf_mask).float())\n",
    "    print(outputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d127b68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Outputs:\")\n",
    "print(outputs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
